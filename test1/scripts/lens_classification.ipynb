{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecce5143",
   "metadata": {},
   "source": [
    "# **Lens Classification with CNN in PyTorch**\n",
    "This notebook trains a CNN model to classify lens images using PyTorch.\n",
    "\n",
    "**Steps:**\n",
    "- Load the dataset\n",
    "- Train a CNN model\n",
    "- Evaluate on validation & test sets\n",
    "- Save the best performing model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8203aea",
   "metadata": {},
   "source": [
    "### **Import Libraries**\n",
    "Import necessary PyTorch libraries, data processing tools, and utility modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e750c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from data_proc import LensDataset\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), 'test1')))\n",
    "from models import *\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aaf1a",
   "metadata": {},
   "source": [
    "### **Load Dataset**\n",
    "Load training and validation datasets using `LensDataset` class. The dataset is split into training (8000 samples) and test sets. A separate validation dataset is also loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80adc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = LensDataset(\"/root/autodl-fs/dataset/train\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [8000, len(dataset) - 8000])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Load validation set\n",
    "valset = LensDataset(\"/root/autodl-fs/dataset/val\")\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0f774",
   "metadata": {},
   "source": [
    "### **Initialize Model**\n",
    "We use a CNN model (`LensCNNClassifier`), `CrossEntropyLoss`, and the Adam optimizer with a learning rate of `2e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb53695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "checkpoint_path = \"/root/autodl-fs/checkpoints/cnn/\"\n",
    "model = LensCNNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3e9fb",
   "metadata": {},
   "source": [
    "### **Define Training&Evaluation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ade9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, device, criterion, data_loader):\n",
    "    '''\n",
    "    Evaluate the model on a test/validation set.\n",
    "\n",
    "    Returns:\n",
    "    - avg_loss (float): Average loss across all batches.\n",
    "    - accuracy (float): Model accuracy.\n",
    "    - mistakes (list): List of (input, true_label, predicted_label) for incorrect predictions.\n",
    "    '''\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    mistakes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # Get predicted class\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            # Record mistakes\n",
    "            for i in range(len(labels)):\n",
    "                if preds[i] != labels[i]:\n",
    "                    mistakes.append((inputs[i].cpu(), labels[i].cpu().item(), preds[i].cpu().item()))\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = tot_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy, mistakes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "best_val_loss = np.inf\n",
    "model_path = \"/root/deeplense/test1/checkpoints/cnn/\"\n",
    "\n",
    "for epoch in range(num_epochs):  # Should overfit within 10 epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent vanishing/exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validate every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        v_loss, v_acc, _ = evaluate_model(model, device, criterion, val_loader)\n",
    "        print(f\"Validate: Epoch {epoch+1}/20, Loss: {v_loss}, Accuracy: {v_acc}\")\n",
    "\n",
    "        if v_loss < best_val_loss:  # Record the best validation model\n",
    "            best_val_loss = v_loss\n",
    "            current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            torch.save(model.state_dict(), f\"{model_path}cnn_{epoch+1}epc_{current_date}.pth\")  # Save model\n",
    "\n",
    "print(\"[INFO] Training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4f3cb",
   "metadata": {},
   "source": [
    "### **Evaluate on Test Set**\n",
    "Once training is complete, evaluate the model on the test dataset and print the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d22dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test Model\n",
    "t_loss, t_acc, _ = evaluate_model(model, device, criterion, test_loader)\n",
    "print(f\"Testing Result: Loss: {t_loss}, Accuracy: {t_acc}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
